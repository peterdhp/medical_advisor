import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAI
from langchain_core.output_parsers import StrOutputParser

from langchain_community.callbacks import get_openai_callback
from menu_streamlit import menu_with_redirect

from langchain_core.pydantic_v1 import BaseModel, Field

from langchain_community.vectorstores import FAISS
from langchain.output_parsers.openai_tools import JsonOutputKeyToolsParser
from langsmith import traceable
import cohere

from operator import itemgetter

from typing import List
from langchain_core.documents import Document
from langchain_openai.embeddings import OpenAIEmbeddings
import os
from langchain_core.runnables import (
    RunnableLambda,
    RunnableParallel,
    RunnablePassthrough,
    RunnableAssign
)


os.environ["CO_API_KEY"] = st.secrets['CO_API_KEY']

if "messages" not in st.session_state:
    st.session_state.messages = []
    
    

def format_docs_with_id(docs: List[Document]) -> str:
    #print(docs)
    formatted = [
        f"source ID : {i}\ntitle : {doc.metadata['title']}\nGuideline snippet :\n{doc.metadata['prechunk']} {doc.page_content} {doc.metadata['postchunk']}"
        for i, doc in enumerate(docs)
    ]
    return "\n\n".join(formatted)

def retrieve_and_merge(query_list : list[str]) -> list[Document] :
    FAISS_PATH = 'faiss' 
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    db = FAISS.load_local(folder_path=FAISS_PATH,embeddings=embeddings,index_name ="CMC_clinical_practice_guideline", allow_dangerous_deserialization=True)
    ### kë¥¼ í†µí•´ ëª‡ê°œì˜ ë¬¸ì„œë¥¼ retrieve í•´ì˜¬ì§€ë¥¼ ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    retriever= db.as_retriever(search_kwargs={'k':10, 'fetch_k':30})
    unique_docs = {}
    for query in query_list:
        result = retriever.invoke(query)
        for doc in result:
            # Using page_content as a unique key to avoid duplicates
            if doc.page_content not in unique_docs:
                unique_docs[doc.page_content] = doc
    docs_list = list(unique_docs.values())
    docs = {x.page_content: i for i, x in enumerate(docs_list)}
    rerank_input = list(docs.keys())
    
    return docs_list, rerank_input

def rerank(rerank_input,question):
    
    co = cohere.Client()
    rerank_response = co.rerank(
        query=question, documents= rerank_input, top_n=5, model="rerank-multilingual-v3.0"
    )
    
    return rerank_response

def compress_retrieve(dict):
    #print(dict["optimizedQuery"])
    query_list = dict["optimizedQuery"].query_list
    question = dict["patient_info"] + dict["question"]
    
    unique_docs, rerank_input = retrieve_and_merge(query_list)
    rerank_response = rerank(rerank_input,question)
    docs = [unique_docs[i.index] for i in rerank_response.results]
    
    return docs
    
class optimizedQuery(BaseModel):
    """list of queries that are optimized to retrieve relevant medical information in respect to the question and patient information"""

    query_list: list[str] = Field(
        description="list of queries that optimized to retrieve relevant medical information in respect to the question and patient information"

    )
    
class Citation(BaseModel):
    source_id: int = Field(
        ...,
        description="The source ID of a SPECIFIC source which justifies the answer.",
    )
    quote: str = Field(
        ...,
        description="The VERBATIM quote from the specified source that justifies the answer.",
    )


class quoted_answer(BaseModel):
    """Answer the user question based only on the given sources, and cite the sources used."""

    answer: str = Field(
        ...,
        description="The answer to the user question, which is based only on the given sources.",
    )
    citations: List[Citation] = Field(
        ..., description="Citations from the given sources that justify the answer."
    )
    
    
def LLM_respond_Q(msg_log):
    """ì±„íŒ… ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ í•¨"""
        
    
    llm = ChatOpenAI(model="gpt-4o", temperature = 0) 
    
    query_system_prompt = [
    ("system", """
You are a specialized medical assistant trained to support doctors by providing accurate and relevant medical information in Korean. When given a patient information, generate a list of Korean queries optimized to retrieve the most appropriate and personalized medical information to answer the last message. 3 queries at max.

[Patient Information]
{patient_info}
""")]
    
    
    
    
    query_prompt = ChatPromptTemplate.from_messages(query_system_prompt+msg_log)
    
    
    query_llm= llm.with_structured_output(optimizedQuery)

    response_system_prompt = [("system", """You are a medical assitant. you are going to assist in medical decision. 
    Answer only based on the given context and patient information.:
    
    [Context]
    {context}
    
    [patient info]
    {patient_info}                   
    """)]

    
    response_prompt = ChatPromptTemplate.from_messages(response_system_prompt + msg_log)
    
    response_llm = llm.bind_tools(
        [quoted_answer],
        tool_choice="quoted_answer",
    )
    output_parser = JsonOutputKeyToolsParser(key_name="quoted_answer", return_single=True)

    chain = (RunnableParallel(patient_info = itemgetter('patient_info'),question = itemgetter('question'))
            .assign(optimizedQuery = itemgetter('patient_info')|query_prompt | query_llm)
            .assign(docs = {"patient_info" : itemgetter("patient_info"), "question" : itemgetter('question'), "optimizedQuery" : itemgetter('optimizedQuery')}| RunnableLambda(compress_retrieve))
            .assign(context = itemgetter('docs') | RunnableLambda(format_docs_with_id))
            .assign(answer = response_prompt | response_llm | output_parser)
            .pick(["docs","answer"]))
    
    
    output = chain.invoke({"patient_info": st.session_state.patient_info, "question" : msg_log[-1][1] })
    #print(output)
    
    return output

    
    
for message in st.session_state.messages:
    role = 'ğŸ©º' if message[0] == 'ai' else message[0]
    with st.chat_message(role):
        st.markdown(message[1])
     
def format_citation_to_text(citations) :   
    text = ''
    for citation in citations :
        text = text + citation.source_title + '\n' + citation.quote + '\n\n'
        
    return text
        
toc = {
    3: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ìˆœí™˜ê¸° ì‘ê¸‰ ì§ˆí™˜',
    8: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ê³ í˜ˆì••',
    17: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ê³ ì§€í˜ˆì¦',
    23: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ì‹¬ë¶€ì „',
    35: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ì‹¬ê·¼ì¦',
    40: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ì‹¬ì¥íŒë§‰ ì§ˆí™˜',
    46: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ì‹¬í˜ˆê´€ ì§ˆí™˜',
    66: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ë¶€ì •ë§¥',
    76: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ë§ì´ˆí˜ˆê´€ì§ˆí™˜ê³¼ íìƒ‰ì „ì¦',
    83: 'ìˆœí™˜ê¸°ë‚´ê³¼ : ìˆ˜ìˆ  ì „ì‹¬ì¥í‰ê°€',
    93: 'í˜¸í¡ê¸°ë‚´ê³¼ : í˜¸í¡ê¸° ì¦ìƒë³„ ì ‘ê·¼ê³¼ ê¸°ëŠ¥ê²€ì‚¬',
    112: 'í˜¸í¡ê¸°ë‚´ê³¼ : ê¸°ê´€ì§€ì²œì‹',
    119: 'í˜¸í¡ê¸°ë‚´ê³¼ : ë§Œì„±íì‡„ì„±íì§ˆí™˜',
    124: 'í˜¸í¡ê¸°ë‚´ê³¼ : íë ´',
    138: 'í˜¸í¡ê¸°ë‚´ê³¼ : ê²°í•µ',
    149: 'í˜¸í¡ê¸°ë‚´ê³¼ : ê°„ì§ˆì„± íì§ˆí™˜',
    165: 'í˜¸í¡ê¸°ë‚´ê³¼ : ê¸°íƒ€í˜¸í¡ê¸°ì§ˆí™˜',
    184: 'í˜¸í¡ê¸°ë‚´ê³¼ : ì¤‘í™˜ìì˜í•™ì´ë¡ ',
    198: 'í˜¸í¡ê¸°ë‚´ê³¼ : í˜¸í¡ë¶€ì „',
    205: 'í˜¸í¡ê¸°ë‚´ê³¼ : ì•Œë ˆë¥´ê¸°ê²€ì‚¬ì™€ ì§ˆí™˜ë³„ ì ‘ê·¼',
    219: 'ì‹ ì¥ë‚´ê³¼ : ì‹ ì¥ì§ˆí™˜ì˜ ì§„ë‹¨',
    229: 'ì‹ ì¥ë‚´ê³¼ : ì „í•´ì§ˆ ë° ì‚°ì—¼ê¸°ì• ',
    251: 'ì‹ ì¥ë‚´ê³¼ : ê¸‰ì„±ì‹ ì¥ì†ìƒ',
    265: 'ì‹ ì¥ë‚´ê³¼ : ë§Œì„±ì‹ ì¥ë³‘ï¼ˆíˆ¬ì„ì „ï¼‰',
    276: 'ì‹ ì¥ë‚´ê³¼ : ì‹ ëŒ€ì²´ìš”ë²•',
    308: 'ì‹ ì¥ë‚´ê³¼ : ì£¼ìš”ì‹ ì¥ì§ˆí™˜',
    342: 'ì‹ ì¥ë‚´ê³¼ : ì„ì‹ ê³¼ì‹ ì¥',
    349: 'ì‹ ì¥ë‚´ê³¼ : ì‹ ì¥ê³¼ì•½ë¬¼',
    361: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ìœ„ì¥ê´€ë‚´ì‹œê²½ê²€ì‚¬',
    369: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ìœ„ì¥ê´€ì¶œí˜ˆ',
    377: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ì‹ë„ì—¼, ì‹ë„ ìš´ë™ì§ˆí™˜ ë° ì‹ë„ì•”',
    386: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ìœ„ì—¼, í—¬ë¦¬ì½”ë°•í„°, ì†Œí™”ì„±ê¶¤ì–‘',
    396: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ìœ„ìš©ì¢… ë° ìœ„ì•”',
    406: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ì„¤ì‚¬, ë³€ë¹„ ë° ê³¼ë¯¼ì„±ì¥ì¦í›„êµ°',
    425: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ì—¼ì¦ì„± ì¥ì§ˆí™˜, ê¸°íƒ€ ì¥ì§ˆí™˜',
    443: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ëŒ€ì¥ìš©ì¢… ë° ëŒ€ì¥ì•”',
    448: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ì·Œì¥ì—¼ ë° ì·Œì¥ë‚­ì¢…ì„±ì§ˆí™˜',
    459: 'ìœ„ì¥ê´€ ë° ì·Œë‹´ë„ : ë‹´ë‚­ ë° ë‹´ë„ì§ˆí™˜',
    469: 'ê°„ì§ˆí™˜ : ê°„ì§ˆí™˜í™˜ìì ‘ê·¼ë²•',
    478: 'ê°„ì§ˆí™˜ : Aí˜• ê°„ì—¼',
    481: 'ê°„ì§ˆí™˜ : Bí˜• ê°„ì—¼',
    488: 'ê°„ì§ˆí™˜ : Cí˜• ê°„ì—¼',
    493: 'ê°„ì§ˆí™˜ : ì•Œì½”ì˜¬ ê°„ì§ˆí™˜ ë° ë¹„ì•Œì½”ì˜¬ ì§€ë°©ê°„ì§ˆí™˜',
    497: 'ê°„ì§ˆí™˜ : ë…ì„±ê°„ì—¼',
    500: 'ê°„ì§ˆí™˜ : ê°„ê²½ë³€ì¦ ë° í•©ë³‘ì¦',
    514: 'ê°„ì§ˆí™˜ : ê¸‰ì„± ê°„ë¶€ì „ê³¼ ê°„ì´ì‹',
    518: 'ê°„ì§ˆí™˜ : ê°„ì•”ê³¼ ê°„ì˜ ì–‘ì„±ì¢…ì–‘',
    528: 'ê°„ì§ˆí™˜ : ìê°€ë©´ì—­ì„±ê°„ì—¼ ë° ëŒ€ì‚¬ì„± ê°„ì§ˆí™˜, ê¸°íƒ€ ê°„ì§ˆí™˜',
    541: 'í˜ˆì•¡ë‚´ê³¼ : í˜ˆì•¡ì§ˆí™˜ í™˜ìì˜ í‰ê°€ ë° ì²˜ì¹˜',
    551: 'í˜ˆì•¡ë‚´ê³¼ : ë¹ˆí˜ˆ',
    559: 'í˜ˆì•¡ë‚´ê³¼ : ê³¨ìˆ˜ë¶€ì „ì¦í›„êµ°',
    570: 'í˜ˆì•¡ë‚´ê³¼ : ê¸‰ì„±ë°±í˜ˆë³‘',
    581: 'í˜ˆì•¡ë‚´ê³¼ : ë§Œì„±ë°±í˜ˆë³‘',
    589: 'í˜ˆì•¡ë‚´ê³¼ : í˜•ì§ˆì„¸í¬ì§ˆí™˜',
    597: 'í˜ˆì•¡ë‚´ê³¼ : ì•…ì„± ë¦¼í”„ì¢…',
    610: 'í˜ˆì•¡ë‚´ê³¼ : ê³¨ìˆ˜ì¦ì‹ì„± ì§ˆí™˜',
    617: 'í˜ˆì•¡ë‚´ê³¼ : ì¶œí˜ˆê³¼ ì§€í˜ˆ',
    628: 'í˜ˆì•¡ë‚´ê³¼ : í˜ˆì•¡ í™˜ìì˜ ìˆ˜í˜ˆ ìš”ë²•',
    636: 'í˜ˆì•¡ë‚´ê³¼ : ì¡°í˜ˆëª¨ì„¸í¬ì´ì‹',
    645: 'ì¢…ì–‘ë‚´ê³¼ : ì¢…ì–‘í™˜ìì˜ ì ‘ê·¼ ë° ì„ìƒì‹œí—˜',
    648: 'ì¢…ì–‘ë‚´ê³¼ : í•­ì•”í™”í•™ìš”ë²•ì˜ ì›ì¹™ ë° ë¶€ì‘ìš©',
    661: 'ì¢…ì–‘ë‚´ê³¼ : íì•”',
    674: 'ì¢…ì–‘ë‚´ê³¼ : ìœ ë°©ì•”',
    682: 'ì¢…ì–‘ë‚´ê³¼ : ìœ„ì•”',
    688: 'ì¢…ì–‘ë‚´ê³¼ : ëŒ€ì¥ì•”',
    693: 'ì¢…ì–‘ë‚´ê³¼ : ê¸°íƒ€ì•” (ë‘ê²½ë¶€ì•”, ì‹ë„ì•”, ì·Œ/ë‹´ë„ì•”, ìœ¡ì¢…/GIST)',
    703: 'ì¢…ì–‘ë‚´ê³¼ : ì›ë°œ ë³‘ì†Œë¶ˆëª…ì•”',
    706: 'ì¢…ì–‘ë‚´ê³¼ : ì§€ì§€ìš”ë²•',
    715: 'ì¢…ì–‘ë‚´ê³¼ : ì¢…ì–‘í•™ì‘ê¸‰',
    725: 'ë‚´ë¶„ë¹„ë‚´ê³¼ : ë‡Œí•˜ìˆ˜ì²´ì§ˆí™˜',
    742: 'ë‚´ë¶„ë¹„ë‚´ê³¼ : ê°‘ìƒì„  ì§ˆí™˜',
    769: 'ë‚´ë¶„ë¹„ë‚´ê³¼ : ë‹¹ë‡¨ë³‘',
    788: 'ë‚´ë¶„ë¹„ë‚´ê³¼ : ì €í˜ˆë‹¹ì¦',
    794: 'ë‚´ë¶„ë¹„ë‚´ê³¼ : ë¶€ì‹  ì§ˆí™˜',
    808: 'ë‚´ë¶„ë¹„ë‚´ê³¼ : ì¹¼ìŠ˜ ë° ê³¨ëŒ€ì‚¬ì§ˆí™˜',
    827: 'ë‚´ë¶„ë¹„ë‚´ê³¼ : ê¸°íƒ€ëŒ€ì‚¬ì§ˆí™˜',
    836: 'ë‚´ë¶„ë¹„ë‚´ê³¼ : ê¸°íƒ€ë‚´ë¶„ë¹„ì§ˆí™˜',
    849: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ê·¼ê³¨ê²©ê³„ì¦ìƒ ì ‘ê·¼ë²•',
    854: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ë¥˜ë§ˆí‹°ìŠ¤ì§ˆí™˜ì§„ë‹¨ê²€ì‚¬',
    858: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ë¥˜ë§ˆí‹°ìŠ¤ì•½ë¬¼',
    865: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ì „ì‹ í™ë°˜ë£¨í‘¸ìŠ¤',
    874: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : í•­ì¸ì§€ì§ˆì¦í›„êµ°',
    876: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ë¥˜ë§ˆí‹°ìŠ¤ê´€ì ˆì—¼',
    881: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ì „ì‹ ê²½í™”ì¦',
    888: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ì‡¼ê·¸ë Œì¦í›„êµ°',
    891: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ê°•ì§ì„±ì²™ì¶”ì—¼',
    899: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : í˜ˆê´€ì—¼ê³¼ ë² ì²´íŠ¸ë³‘',
    909: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ì—¼ì¦ì„±ê·¼ë³‘ì¦',
    915: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ê³¨ê´€ì ˆì—¼',
    920: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : í†µí’',
    924: 'ë¥˜ë§ˆí‹°ìŠ¤ë‚´ê³¼ : ì„¬ìœ ê·¼í†µ',
    931: 'ê°ì—¼ë‚´ê³¼ : ê°ì—¼ì§ˆí™˜ì˜ ì§„ë‹¨',
    938: 'ê°ì—¼ë‚´ê³¼ : ê°ì—¼ì§ˆí™˜ì¹˜ë£Œ',
    975: 'ê°ì—¼ë‚´ê³¼ : ì£¼ìš” ê°ì—¼ì§ˆí™˜',
    1055: 'ê°ì—¼ë‚´ê³¼ : ê°ì—¼ì§ˆí™˜ì˜ ì˜ˆë°©'
}
        
def get_key_from_value(d, value):
    for k, v in d.items():
        if v == value:
            return k
    return None       
        
if userinput := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”"):
    # Add user message to chat history
    st.session_state.messages.append(("human", userinput))
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(userinput)
    with st.chat_message("ğŸ©º"):
        output = LLM_respond_Q(st.session_state.messages)
        #print(output)
        answer = output['answer'][0]['answer']
        citations = output['answer'][0]['citations']
        docs = output['docs']
        st.markdown(answer)
    st.session_state.messages.append(("ai", answer))
    for i, citation in enumerate(citations):
        with st.expander(citation['quote']):
            st.markdown(docs[citation['source_id']].metadata['title']+ "        CMC ë‚´ê³¼ì§„ë£Œì§€ì¹¨ì„œ "+ str(get_key_from_value(toc, docs[citation['source_id']].metadata['title']))+"ìª½")
            st.markdown(f"{docs[citation['source_id']].metadata['prechunk']} {docs[citation['source_id']].page_content} {docs[citation['source_id']].metadata['postchunk']}")
            
            
    


    
    
menu_with_redirect()
